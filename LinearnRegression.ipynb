{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddf2c5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be included at the beginning of each new notebook. Remember to change the app name.\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a70e57c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('data/transformed.csv',inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "779e733d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- budget: integer (nullable = true)\n",
      " |-- popularity: double (nullable = true)\n",
      " |-- revenue: integer (nullable = true)\n",
      " |-- runtime: integer (nullable = true)\n",
      " |-- vote_count: integer (nullable = true)\n",
      " |-- genre: string (nullable = true)\n",
      " |-- release_date: string (nullable = true)\n",
      " |-- production_country: string (nullable = true)\n",
      " |-- popularity_rank: string (nullable = true)\n",
      " |-- risk: string (nullable = true)\n",
      "\n",
      "+---------+----------+----------+-------+----------+---------------+------------+------------------+---------------+----+\n",
      "|   budget|popularity|   revenue|runtime|vote_count|          genre|release_date|production_country|popularity_rank|risk|\n",
      "+---------+----------+----------+-------+----------+---------------+------------+------------------+---------------+----+\n",
      "|300000000|139.082615| 961000000|    169|      4500|         Action|  2007-05-19|      united_state|           high| low|\n",
      "|245000000|107.376788| 880674609|    148|      4466|         Action|  2015-10-26|            others|           high| low|\n",
      "|250000000| 112.31295|1084939099|    165|      9106|         Action|  2012-07-16|      united_state|           high| low|\n",
      "|260000000| 43.926995| 284139100|    132|      2124|         Action|  2012-03-07|      united_state|           high| low|\n",
      "|260000000| 48.681969| 591794936|    100|      3330|      Animation|  2010-11-24|      united_state|           high| low|\n",
      "|280000000|134.279229|1405403694|    141|      6767|         Action|  2015-04-22|      united_state|           high| low|\n",
      "|250000000| 98.885637| 933959197|    153|      5293|      Advanture|  2009-07-07|            others|           high| low|\n",
      "|250000000|155.790452| 873260194|    151|      7004|         Action|  2016-03-23|      united_state|           high| low|\n",
      "|270000000| 57.925623| 391081192|    154|      1400|         Action|  2006-06-28|      united_state|           high| low|\n",
      "|200000000|107.928811| 586090727|    106|      2965|      Advanture|  2008-10-30|            others|           high| low|\n",
      "|200000000|145.847379|1065659812|    151|      5246|      Advanture|  2006-06-20|            others|           high| low|\n",
      "|255000000| 49.046956|  89289910|    149|      2311|         Action|  2013-07-03|      united_state|           high|high|\n",
      "|225000000| 99.398009| 662845518|    143|      6359|         Action|  2013-06-12|            others|           high| low|\n",
      "|225000000| 53.978602| 419651413|    150|      1630|      Advanture|  2008-05-15|            others|           high| low|\n",
      "|220000000|144.448633|1519557910|    143|     11776|Science fiction|  2012-04-25|      united_state|           high| low|\n",
      "|380000000|135.413856|1045713802|    136|      4948|      Advanture|  2011-05-14|      united_state|           high| low|\n",
      "|225000000| 52.035179| 624026776|    106|      4160|         Action|  2012-05-23|      united_state|           high| low|\n",
      "|250000000|120.965743| 956019788|    144|      4760|         Action|  2014-12-10|            others|           high| low|\n",
      "|215000000| 89.866276| 752215857|    136|      6586|         Action|  2012-06-27|      united_state|           high| low|\n",
      "|200000000| 37.668301| 310669540|    140|      1398|         Action|  2010-05-12|            others|           high| low|\n",
      "+---------+----------+----------+-------+----------+---------------+------------+------------------+---------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Optionally, rename columns for better clarity\n",
    "df = df.withColumnRenamed(\"_c0\", \"budget\") \\\n",
    "       .withColumnRenamed(\"_c1\", \"popularity\") \\\n",
    "       .withColumnRenamed(\"_c2\", \"revenue\") \\\n",
    "       .withColumnRenamed(\"_c3\", \"runtime\") \\\n",
    "       .withColumnRenamed(\"_c4\", \"vote_count\") \\\n",
    "       .withColumnRenamed(\"_c5\", \"genre\") \\\n",
    "       .withColumnRenamed(\"_c6\", \"release_date\") \\\n",
    "       .withColumnRenamed(\"_c7\", \"production_country\")\\\n",
    "       .withColumnRenamed(\"_c8\", \"popularity_rank\")\\\n",
    "       .withColumnRenamed(\"_c9\", \"risk\")\n",
    "# Let's get an idea of what the data looks like. \n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "380f10dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "# Convert categorical columns into numerical representations\n",
    "indexer_risk = StringIndexer(inputCol=\"risk\", outputCol=\"risk_index\")\n",
    "\n",
    "df = indexer_risk.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4b2fb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+-------+----------+---------------+------------+------------------+---------------+----+----------+\n",
      "|   budget|popularity|   revenue|runtime|vote_count|          genre|release_date|production_country|popularity_rank|risk|risk_index|\n",
      "+---------+----------+----------+-------+----------+---------------+------------+------------------+---------------+----+----------+\n",
      "|300000000|139.082615| 961000000|    169|      4500|         Action|  2007-05-19|      united_state|           high| low|       0.0|\n",
      "|245000000|107.376788| 880674609|    148|      4466|         Action|  2015-10-26|            others|           high| low|       0.0|\n",
      "|250000000| 112.31295|1084939099|    165|      9106|         Action|  2012-07-16|      united_state|           high| low|       0.0|\n",
      "|260000000| 43.926995| 284139100|    132|      2124|         Action|  2012-03-07|      united_state|           high| low|       0.0|\n",
      "|260000000| 48.681969| 591794936|    100|      3330|      Animation|  2010-11-24|      united_state|           high| low|       0.0|\n",
      "|280000000|134.279229|1405403694|    141|      6767|         Action|  2015-04-22|      united_state|           high| low|       0.0|\n",
      "|250000000| 98.885637| 933959197|    153|      5293|      Advanture|  2009-07-07|            others|           high| low|       0.0|\n",
      "|250000000|155.790452| 873260194|    151|      7004|         Action|  2016-03-23|      united_state|           high| low|       0.0|\n",
      "|270000000| 57.925623| 391081192|    154|      1400|         Action|  2006-06-28|      united_state|           high| low|       0.0|\n",
      "|200000000|107.928811| 586090727|    106|      2965|      Advanture|  2008-10-30|            others|           high| low|       0.0|\n",
      "|200000000|145.847379|1065659812|    151|      5246|      Advanture|  2006-06-20|            others|           high| low|       0.0|\n",
      "|255000000| 49.046956|  89289910|    149|      2311|         Action|  2013-07-03|      united_state|           high|high|       1.0|\n",
      "|225000000| 99.398009| 662845518|    143|      6359|         Action|  2013-06-12|            others|           high| low|       0.0|\n",
      "|225000000| 53.978602| 419651413|    150|      1630|      Advanture|  2008-05-15|            others|           high| low|       0.0|\n",
      "|220000000|144.448633|1519557910|    143|     11776|Science fiction|  2012-04-25|      united_state|           high| low|       0.0|\n",
      "|380000000|135.413856|1045713802|    136|      4948|      Advanture|  2011-05-14|      united_state|           high| low|       0.0|\n",
      "|225000000| 52.035179| 624026776|    106|      4160|         Action|  2012-05-23|      united_state|           high| low|       0.0|\n",
      "|250000000|120.965743| 956019788|    144|      4760|         Action|  2014-12-10|            others|           high| low|       0.0|\n",
      "|215000000| 89.866276| 752215857|    136|      6586|         Action|  2012-06-27|      united_state|           high| low|       0.0|\n",
      "|200000000| 37.668301| 310669540|    140|      1398|         Action|  2010-05-12|            others|           high| low|       0.0|\n",
      "+---------+----------+----------+-------+----------+---------------+------------+------------------+---------------+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47af7f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, udf, min, max\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"LinearRegressionExample\").getOrCreate()\n",
    "\n",
    "\n",
    "# Calculate min and max values for revenue and budget\n",
    "revenue_min, revenue_max = df.select(min(\"revenue\"), max(\"revenue\")).first()\n",
    "budget_min, budget_max = df.select(min(\"budget\"), max(\"budget\")).first()\n",
    "\n",
    "# Define UDF for Min-Max scaling\n",
    "def min_max_scaling(value, min_value, max_value):\n",
    "    return (value - min_value) / (max_value - min_value)\n",
    "\n",
    "# Define UDF for inverse Min-Max scaling\n",
    "def inverse_min_max_scaling(scaled_value, min_value, max_value):\n",
    "    return (scaled_value * (max_value - min_value)) + min_value\n",
    "\n",
    "min_max_scaling_udf = udf(lambda x: min_max_scaling(x, revenue_min, revenue_max), DoubleType())\n",
    "inverse_min_max_scaling_udf = udf(lambda x: inverse_min_max_scaling(x, budget_min, budget_max), DoubleType())\n",
    "\n",
    "# Apply Min-Max scaling to revenue and budget columns\n",
    "df = df.withColumn(\"revenue_scaled\", min_max_scaling_udf(col(\"revenue\")))\n",
    "df = df.withColumn(\"budget_scaled\", min_max_scaling_udf(col(\"budget\")))\n",
    "\n",
    "# List of feature column names\n",
    "all_feature_columns = ['risk_index', 'revenue_scaled']\n",
    "\n",
    "# Assemble features into a single vector\n",
    "assembler = VectorAssembler(inputCols=all_feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(df)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# Create and train the Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"budget_scaled\")\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Calculate R-squared\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"budget_scaled\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "\n",
    "print(f\"R-squared (R2) = {r2}\")\n",
    "\n",
    "# Print the coefficients and intercept of the model\n",
    "print(f\"Coefficients: {model.coefficients}\")\n",
    "print(f\"Intercept: {model.intercept}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9adeb80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/21 07:40:21 WARN Instrumentation: [dff30f9b] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) = 31091547.939098485\n",
      "R-squared (R2) = 0.6592831157869041\n",
      "Coefficients: [9801590.630461901,0.1952494728034208]\n",
      "Intercept: 18863729.323349126\n",
      "Formula: budget = 18863729.323349126 + (9801590.630461901 * risk_index) + (0.1952494728034208 * revenue)\n",
      "+-----------------+-------+--------------------+\n",
      "|         features| budget|          prediction|\n",
      "+-----------------+-------+--------------------+\n",
      "|[0.0,1.0178331E7]| 100000| 2.085104308511784E7|\n",
      "| [0.0,4.266441E7]| 100000| 2.719393288331812E7|\n",
      "|[0.0,3.3456317E7]| 500000| 2.539605757954325E7|\n",
      "|   [1.0,171760.0]| 500000| 2.869885600325974E7|\n",
      "|   [1.0,171760.0]| 500000| 2.869885600325974E7|\n",
      "|  [0.0,4242978.0]| 500000| 1.969216854096564E7|\n",
      "|  [0.0,4242978.0]| 500000| 1.969216854096564E7|\n",
      "|  [0.0,1185783.0]| 852510|1.9095252828958385E7|\n",
      "|[0.0,1.0047674E7]|1000000|2.0825532374749765E7|\n",
      "|   [1.0,444575.0]|1000000|2.8752122988182608E7|\n",
      "|  [0.0,4235151.0]|1000000|1.9690640323342007E7|\n",
      "|  [0.0,4105187.0]|1100000| 1.966526492085858E7|\n",
      "|  [0.0,2861020.0]|1200000|1.9422341970029168E7|\n",
      "|   [1.0,639000.0]|1300000|2.8790084366932414E7|\n",
      "|  [1.0,1165882.0]|1500000|2.8892957799662024E7|\n",
      "|   [0.0,3.0448E7]|1500000|2.4808685271267682E7|\n",
      "| [0.0,1.915248E7]|1987650|2.2603240946227185E7|\n",
      "|   [1.0,841733.0]|2000000| 2.882966787830227E7|\n",
      "|   [1.0,622806.0]|2000000|2.8786922496969834E7|\n",
      "|  [0.0,5048693.0]|2000000|1.9849483969945446E7|\n",
      "+-----------------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"LinearRegressionExample\").getOrCreate()\n",
    "\n",
    "# List of feature column names\n",
    "all_feature_columns = ['risk_index', 'revenue']\n",
    "\n",
    "# Assemble features into a single vector\n",
    "assembler = VectorAssembler(inputCols=all_feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(df)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# Create and train the Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"budget\")\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"budget\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "\n",
    "# Calculate R-squared\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"budget\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE) = {rmse}\")\n",
    "print(f\"R-squared (R2) = {r2}\")\n",
    "\n",
    "# Print the coefficients and intercept of the model\n",
    "print(f\"Coefficients: {model.coefficients}\")\n",
    "print(f\"Intercept: {model.intercept}\")\n",
    "\n",
    "# Print the formula of the linear regression model\n",
    "feature_coefficients = list(zip(all_feature_columns, model.coefficients))\n",
    "formula = f\"budget = {model.intercept}\"\n",
    "for feature, coef in feature_coefficients:\n",
    "    formula += f\" + ({coef} * {feature})\"\n",
    "print(f\"Formula: {formula}\")\n",
    "\n",
    "# Show the predictions\n",
    "predictions.select(\"features\", \"budget\", \"prediction\").show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b6ee547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/21 07:57:53 WARN Instrumentation: [165f36be] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) = 0.6592831157869041\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"LinearRegressionExample\").getOrCreate()\n",
    "\n",
    "# List of feature column names\n",
    "all_feature_columns = ['risk_index', 'revenue']\n",
    "\n",
    "# Assemble features into a single vector\n",
    "assembler = VectorAssembler(inputCols=all_feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(df)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# Create and train the Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"budget\")\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Calculate R-squared\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"budget\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "\n",
    "print(f\"R-squared (R2) = {r2}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
